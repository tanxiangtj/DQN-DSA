# main configuration to setup the network
# usage - call by multiNodeLearning.py file
# to save different setting to run expect results



# indent ? (T.B.D)


[Global]


[dumbNodes]



[learningNodes]

[Networks]
# ToDo Legacy with Random <<<<

# The type of each node 
# 0 'legacy'  - Legacy (Dumb) Node 
# 1 'hopping' - Hopping Node
# 2 'im'      - Intermittent Node/ im Node
# 3 'dsa'     - DSA node (just avoids)  
# 4 'possion' - possion Node
    
# 10 'mdp'          - MDP Node
# 11 'dqn'          - a. DQN Node
# 12 'dqnDouble'    - b. DQN-DoubleQ
# 13 'dqnPriReplay' - c. DQN-PriReplay
# 14 'dqnDuel'      - d. DQN-Duel   
# 15 'dqnRef'       - e. DQ-Refined
# 16 'dpg'          - DPG policy gredient
# 17 'ac'           - Actor Critic
# 18 'ddpg'         - Distributed Proximal Policy Optimization  (T.B.D)
# 19 'a3cDiscrete'  - A3C discrete action
# 20 'a3cDistribute'
# 21 'a3cRNN'
# 22 'dqnDynamic'
# 22 'dppo'    
    
# 30 - pomcp



numSteps = 10
numChans = 6
ChannelAssignType = typeIn  


# nodeTypes = [0, 0, 18]

#nodeTypes = [0, 0, 10, 11,12,13,14,15,16]


nodeTypes = [ 0,1,2,3,4,
              10,11,12,13,14,15,16,17]

legacyChanList   = [0, 1]
txProbability    = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
hoppingChanList  = [ [1, 2] ]
hoppingWidth     = 2
hopRate     = 10
imChanList       = [3]                      
imDutyCircleList = [0.2, 0.8] 

poissonChanList  = 4 
arrivalRate       = 10
serviceRate      = 8


# test case to test "learning hidden" nodeTypes = [0,0,5,5] where 3rd & 4th is duplex conflict
# test case to test "learning spatial" nodeTypes = [0,0,0,5,5] where 4rd & 5th is duplex conflict


[Neural Network]
# in dqnxx.py mdp.py
# Set "explore ratio" - exploreTye, exploreDecay, exploreProbMin
# set "reward"

