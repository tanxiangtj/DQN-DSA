# main configuration to setup the network
# usage - run multiNodeLearning.py file
# to save different setting to run expect results

# The type of each node 
# 0 'legacy'  - Legacy (Dumb) Node 
# 1 'hopping' - Hopping Node
# 2 'im'      - Intermittent Node/ im Node
# 3 'dsa'     - DSA node (just avoids)  
# 4 'possion' - possion Node
# 5 'markovChain' - markovChain Node 
    
# 10 'mdp'          - MDP Node
# 11 'dqn'          - a. DQN Node
# 12 'dqnDouble'    - b. DQN-DoubleQ
# 13 'dqnPriReplay' - c. DQN-PriReplay
# 14 'dqnDuel'      - d. DQN-Duel   
# 15 'dqnRef'       - e. DQ-Refined
# 16 'dpg'          - DPG policy gradient
# 17 'ac'           - Actor Critic
# 18 'ddpg'         - Distributed Proximal Policy Optimization  (T.B.D)
# 19 'a3cDiscrete'  - A3C discrete action
# 20 'a3cDistribute'
# 21 'a3cRNN'
# 22 'dqnDynamic'
# 23 'dppo' 
# 24 'et'           - eligiable trace
# 25 'guess'        - memory with guess sample
    
# partial obervation    
# 30 'dqnPad'       - pad till full observation DQN 
# 31 'dqnPo'        - shorten partial observation DQN 
# 32 'dqnStack'     - stacked partial obervation as input to DQN
# 33 'dpgStack'     - stacked partial obervation as input to DQN
#---------------------------
# 34 'drqn'         - deep recurrent q network



[Global]
numSteps  = 30000
numChans  = 4
optimalTP = 4

###################   type of node #################################
nodeTypes = [0,0,34,34]
####################################################################



[legacyNode]
legacyChanList   = [0,1]
# could be NOT 1.0, e.g. 0.1


[hoppingNode]
hoppingChanList  = [ [2,3]]
hoppingWidth     = 2
hopRate          = 1
offSet           = [50]
# the hop rate matters    


[imNode]
imChanList       = [1]                      
#imDutyCircleList = [[0.1, 0.2, 0.5, 0.9]] 
imDutyCircleList = [[0.50]]
imPeriod         = 4

[poissonNode]
poissonChanList  = 2 
arrivalInterval      = 5
serviceInterval      = 4

[markovChainNode]
mcChanList  = 7  
alpha       = 0.1
beta        = 0.9

[noise]
noiseErrorProb   = 0.00
noiseFlipNum     = 1

[partialObservation]

# for pad 
poBlockNum = 4
# for partial observation   
poSeeNum   = 2  
poStepNum  = 2

padEnable = 0

padValue = 1
stackNum = 1


# To Do Pad Value

[Neural Network]
# in dqnxx.py mdp.py
# Set "explore ratio" - exploreTye, exploreDecay, exploreProbMin
# set "reward"

